# Review of key points from EH 6124

These are the major points from EH6124: Introduction to Clinical Trial Design and Analysis. Please ask any questions on the discussion board if you need to clarify anything, or even if you just want to argue with me.

## The role of the statistician

Successful clinical trials require teams with a variety of expertise. Though many scientists and clinical investigators receive some training in statistics and study design, this training is usually rudimentary, and often clumsy. The trial statistician is thus needed to provide expert guidance in these areas, and so should be heavily involved in the design, analysis, and reporting phases of the project. However, to do their job properly, the statistician must rely on the subject matter expertise of the clinical investigators. Thus the scientific goals of clinical trials are best met when there are close collaborations between clinical investigators and statisticians.

## Causal inference

Most clinical studies estimate and report an association between an outcome (e.g. mortality) and an exposure (such being randomly allocated to one study arm or another). Usually we want to make some inferences about that estimated association, to help us decide whether the exposure caused the outcome, i.e. would the observed distribution of the outcome have been different had there been no exposure? Our confidence in any causal inferences largely rests on an assumption of “no confounding” which means there aren’t any other extraneous factors that cause both the exposure and the outcome. This can vaguely-stated condition can be explicitly described using Directed Acyclic Graphs, which we can use to show that if an exposure is indeed randomly allocated, then there can be no confounding. Thus randomization allows us to make causal inferences with the fewest/most-palatable assumptions, compared to other methods (e.g. adjusting/controlling for suspected confounders).  

## Regression to the mean

Patients often enter a clinical trial when they are ill, and most illnesses have some natural variability in their progression. This means that if I recruit a sample of sick people and do nothing, some will get better anyway, and thus the health of the group will have improved, on average. This is called regression to the mean. Unfortunately, many researchers set up similar scenarios, except they do intervene in some way, and then mistakenly attribute the improvement in their sample’s average health as an effect of intervention. This is why we include concurrent control groups in RCTs. Then we can get an estimate of the magnitude of regression to the mean (from the control group), which we can then subtract from our estimated treatment effect in the active arm by literally looking at the difference in outcomes between the study arms. This doesn’t mean we can’t ever make inferences from within-group comparisons (pre vs post) or using “historical” controls (control groups external to our trial), but we should do so much more cautiously, and not expect many other people to trust our results even if we do.

## Equipoise and ethics

When we run a RCT, we are exposing patients to unknown risks in the hopes of learning something important about treatment that might benefit future patients, as well as those enrolled in the trial. For this to be ethical, it means that at a minimum, all patients enrolled in the trial must get at least the same quality of care as they would have if they hadn’t enrolled in the trial. It also means that there should be a genuine uncertainty about the potential benefits of the new intervention – that is that we should be in a state of equipoise. Equipoise can be hard to demonstrate, so it’s important for investigators to clearly make their case that it exists (though this often doesn’t happen). In my opinion, this is an area where patient voices should have more prominence. The ethical obligations of the trial also mean that we should stand to learn something important, which means that shoddy comparators (known to be substandard, thus stacking the deck in favour of the new treatment) and other preventable design flaws are arguably unethical.

## Patient selection

We must carefully consider which patients to enroll on our RCT. This will largely depend on the overall goals of the trial. In the initial phase 3 trials for a new treatment, we are often most interested in seeing if the intervention can work in a “best-case” scenario. This means that we select patients in a manner that maximizes the internal validity of the trial, with little or no consideration for the external validity or generalizability. In turn, this means recruiting patients that are most likely to benefit from, and adhere to, the proposed treatment (based on our current understanding). It also means recruiting a more homogeneous sample, to reduce natural variability in the outcome. This will make it easier to see the effects of our intervention, if there is one, using a smaller sample than would be required to see the same effect in a broader, more heterogeneous sample. This best-case scenario makes sense for newly tested treatments, since it’s less costly (less money, fewer patients exposed unknown risks), and because if we fail to see an effect under these favourable conditions, it’s probably safe to conclude that we can move on from this intervention to pursue other possibilities.

That said, once we’ve demonstrated the efficacy of an intervention in trials with a high internal validity (i.e. we’ve demonstrated that the intervention can work), we will like want to see if it does work when applied in something that looks more like normal clinical practice. This is where effectiveness (or pragmatic) trials come in, where we want more broadly representative samples and scenarios. The implication is that there might be practical issues with compliance in a broader sample, or that there might be heterogeneity in treatment effects (HTE). This means that some groups of patients will benefit more or less from the intervention than other groups, and that these different groups might have been disproportionately represented in the earlier efficacy trials we described above. Unfortunately, the sample size required to demonstrate such interactions can be much larger than that needed to demonstrate the marginal (on average) effect of the treatment, so frequently concerns about HTE are not evidenced. Regardless, demonstrating a beneficial average treatment effect in a more generalizable sample is still very comforting, especially to people who make health technology assessments (i.e. the people who decide which treatments to fund with public money).

Importantly, even if we have reason to expect no HTE, we want to be mindful of representation. This means working to ensure that all patients have access to participate in clinical trials, as patients enrolled on trials, even when the new treatment doesn’t out-perform standard care, often have better outcomes than those who aren’t enrolled. Poor representation (for example, of women or ethnic minorities) can also degraded trust (and rightly so) in clinical research overall.

Lastly, we control the composition of the patient sample with inclusion and exclusion criteria. A common mistake is to make a list where the list exclusions are just the conversely-stated inclusion criteria (e.g. inclusion: age >= 65 years; exclusion: age < 65 years). It’s better to consider inclusion as controlling entry into the trial to get a more/less homogeneous sample, and to precisely define the disease/problem we are trying to impact.  Exclusions then, which should usually be much fewer in number, are typically used to excluded people that can’t consent, that aren’t expected to possibly benefit from the new treatment, or that might be a higher than acceptable risk (in either arm). Patients are often excluded based on flawed reasoning about homogeneity (presumably to increase the internal validity of the trial), which can only be correctly considered with respect to factors that predict variability in the outcome, i.e. there is zero point in excluding patients based on some demographic or other factor if it has no relationship to the outcome.

## Outcomes

We define the effects of interventions in terms of their impact on outcomes. In other words, outcomes are the variables we want to change in response to an intervention. So first and foremost, we must ensure that the outcomes we use in a trial are actually important, and be cautious about using so-called surrogate outcomes, which may reflect that the intervention did something, but not necessarily what we wanted it to do. Outcomes must also be precisely defined and of course measurable. Your description of an outcome should also avoid any qualitative statements (e.g. systolic blood pressure is an outcome, while “improved” systolic blood pressure is not).

Our choice of outcomes will also have important implications for the overall design of the trial. Generally, outcome that are noisier (have more natural variance, which is typical of subjective measures), or rarer, will require a greater sample of patients to demonstrate the effect of an intervention. That said, we must often accommodate this if those in fact are the most important outcomes. However, one avoidable but still frequently made mistake is the categorization of inherently continuous outcomes, which always results in a loss of information and needlessly lowers the power of the study (and even if some categorized outcome is perceived as being more relevant, this can always be captured from the analysis of the underlying continuous outcome).

## Covariates

There is some disagreement among trialists about how to treat covariate information. In general, model based adjustment for strong predictors of the outcome will result in a more efficient (more powerful) estimator of the treatment effect. This means we don’t need to enroll as many patients on the trial to detect the minimally important effect size (or we have even higher power to detect this effect with the same number of patients). This part is uncontroversial. However, some people see any covariate adjustment as a problem, especially if they suspect that the choice of covariates to adjust for is made after seeing the data, with the intent to produce a small p-value for the effect of the intervention (p-hacking). Other people are fine with covariate adjustment, but they choose their covariates based on perceived imbalances in the covariate distributions between trial arms (so called “table 1 tests”). However, this procedure is sub-optimal, recommended against by all competent authorities, and opens the investigator up to the accusations of p-hacking we just discussed.  

The correct way to account for covariate information is to use your subject matter expertise and understanding of the outcome to select the strongest prognostic factors before the study begins, and to pre-register� these decisions in the statistical analysis plan attached to the clinical trial registration. Then the reported analyses at the end of the study much match what was declared in the registration (and thus couldn’t have been p-hacked).

The final point is how to handle baseline information. The baseline is a measure of the outcome that is taken prior to randomization. Baselines are thus often powerful predictors of the later outcome and thus a good choice for model based adjustment. However, instead of this, some investigators calculate change scores (outcome minus baseline) and use that for the outcome in the eventual trial analysis.  What they don’t realize is that such a change score will still be correlated with the baseline values (but now in the opposite direction), and thus still benefit from an adjustment for baseline – and that the estimated effect of an intervention on the change score adjusted for baseline will be exactly the same as that on the (raw) outcome adjusted for baseline. While there are some scenarios where the unadjusted estimator of the treatment effect will be more efficient using change scores vs raw outcome, the baseline adjusted estimator is always more efficient than unadjusted change scores.